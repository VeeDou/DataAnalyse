{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ•°æ®æŒ–æ˜å¤§è‡´æµç¨‹\n",
    "1. æ•°æ®æ¢ç´¢æ€§åˆ†æ \n",
    "2. ç‰¹å¾å·¥ç¨‹\n",
    "3. å»ºæ¨¡è°ƒå‚\n",
    "4. **æ¨¡å‹èåˆâ€”â€”â€”â€”â€”â€”ğŸ‘ˆ**\n",
    "\n",
    "## å½“å‰é˜¶æ®µç›®æ ‡â€”â€”å¯¹äºå¤šç§è°ƒå‚å®Œæˆçš„æ¨¡å‹è¿›è¡Œæ¨¡å‹èåˆ\n",
    "1. ç†è§£3ä¸­æ¨¡å‹èåˆæ–¹æ³•\n",
    "2. å®Œæˆèåˆç»“æœå¹¶æ‰“å¡\n",
    "\n",
    "## æ¨¡å‹èåˆæ€»ç»“\n",
    "1. ç¬¬ä¸€ç§æ–¹æ³•ï¼Œç®€å•åŠ æƒèåˆ\n",
    "    - å›å½’é—®é¢˜çš„é¢„æµ‹æ•°å€¼ã€åˆ†ç±»çš„æ¦‚ç‡ç›´æ¥è®¡ç®—å‡å€¼ï¼Œä¾‹å¦‚ç®—æ•°å¹³å‡å’Œé›†åˆå¹³å‡ï¼Œç„¶åå–æœ€å¹³å‡é¢„æµ‹æ•°å€¼æˆ–è€…æœ€å¤§é¢„æµ‹æ¦‚ç‡çš„é‚£ä¸ªåˆ†ç±»ã€‚\n",
    "    - åˆ†ç±»é—®é¢˜ç›´æ¥è¿›è¡ŒæŠ•ç¥¨ï¼Œå½“ç„¶ä¹Ÿå¯ä»¥è¿›è¡ŒåŠ æƒ\n",
    "    - ç»¼åˆï¼Ÿï¼Ÿè¿˜æ²¡æ˜ç™½\n",
    "2. ç¬¬äºŒç§æ–¹æ³•ï¼Œstacking/blendingï¼Œé€šè¿‡æ„å»ºå¤šå±‚æ¨¡å‹å¹¶åˆ©ç”¨é¢„æµ‹ç»“æœå†æ‹Ÿåˆé¢„æµ‹ã€‚ï¼ˆæ²¡æ˜ç™½ï¼Œä¸ºä»€ä¹ˆè¦ç”¨å¤šå±‚æ¨¡å‹ï¼Œä¹Ÿæ²¡æ˜ç™½æ€ä¹ˆç”¨ï¼‰\n",
    "3. boosting/bagging å¤šæ ‘çš„èåˆï¼Œåœ¨xgboostå’ŒAdaboost,GBDTä¸­å·²ç»ç”¨åˆ°\n",
    "\n",
    "> å¤§ä½¬çš„ç»éªŒæ€»ç»“\n",
    "> æ¯”èµ›çš„èåˆè¿™ä¸ªé—®é¢˜ï¼Œä¸ªäººçš„çœ‹æ³•æ¥è¯´å…¶å®æ¶‰åŠå¤šä¸ªå±‚é¢ï¼Œä¹Ÿæ˜¯æåˆ†å’Œæå‡æ¨¡å‹é²æ£’æ€§çš„ä¸€ç§é‡è¦æ–¹æ³•ï¼š    \n",
    "> 1ï¼‰**ç»“æœå±‚é¢çš„èåˆ**ï¼Œè¿™ç§æ˜¯æœ€å¸¸è§çš„èåˆæ–¹æ³•ï¼Œå…¶å¯è¡Œçš„èåˆæ–¹æ³•ä¹Ÿæœ‰å¾ˆå¤šï¼Œæ¯”å¦‚æ ¹æ®ç»“æœçš„å¾—åˆ†è¿›è¡ŒåŠ æƒèåˆï¼Œè¿˜å¯ä»¥åšLogï¼Œexpå¤„ç†ç­‰ã€‚åœ¨åšç»“æœèåˆçš„æ—¶å€™ï¼Œæœ‰ä¸€ä¸ªå¾ˆé‡è¦çš„æ¡ä»¶æ˜¯æ¨¡å‹ç»“æœçš„å¾—åˆ†è¦æ¯”è¾ƒè¿‘ä¼¼ï¼Œç„¶åç»“æœçš„å·®å¼‚è¦æ¯”è¾ƒå¤§ï¼Œè¿™æ ·çš„ç»“æœèåˆå¾€å¾€æœ‰æ¯”è¾ƒå¥½çš„æ•ˆæœæå‡ã€‚    \n",
    "> 2ï¼‰**ç‰¹å¾å±‚é¢çš„èåˆ**ï¼Œè¿™ä¸ªå±‚é¢å…¶å®æ„Ÿè§‰ä¸å«èåˆï¼Œå‡†ç¡®è¯´å¯ä»¥å«åˆ†å‰²ï¼Œå¾ˆå¤šæ—¶å€™å¦‚æœæˆ‘ä»¬ç”¨åŒç§æ¨¡å‹è®­ç»ƒï¼Œå¯ä»¥æŠŠç‰¹å¾è¿›è¡Œåˆ‡åˆ†ç»™ä¸åŒçš„æ¨¡å‹ï¼Œç„¶ååœ¨åé¢è¿›è¡Œæ¨¡å‹æˆ–è€…ç»“æœèåˆæœ‰æ—¶ä¹Ÿèƒ½äº§ç”Ÿæ¯”è¾ƒå¥½çš„æ•ˆæœã€‚    \n",
    "> 3ï¼‰**æ¨¡å‹å±‚é¢çš„èåˆ**ï¼Œæ¨¡å‹å±‚é¢çš„èåˆå¯èƒ½å°±æ¶‰åŠæ¨¡å‹çš„å †å å’Œè®¾è®¡ï¼Œæ¯”å¦‚åŠ Stakingå±‚ï¼Œéƒ¨åˆ†æ¨¡å‹çš„ç»“æœä½œä¸ºç‰¹å¾è¾“å…¥ç­‰ï¼Œè¿™äº›å°±éœ€è¦å¤šå®éªŒå’Œæ€è€ƒäº†ï¼ŒåŸºäºæ¨¡å‹å±‚é¢çš„èåˆæœ€å¥½ä¸åŒæ¨¡å‹ç±»å‹è¦æœ‰ä¸€å®šçš„å·®å¼‚ï¼Œç”¨åŒç§æ¨¡å‹ä¸åŒçš„å‚æ•°çš„æ”¶ç›Šä¸€èˆ¬æ˜¯æ¯”è¾ƒå°çš„ã€‚      \n",
    "> from  ML67  \n",
    ">         Email: maolinw67@163.com\n",
    ">         PS: åä¸­ç§‘æŠ€å¤§å­¦ç ”ç©¶ç”Ÿ, é•¿æœŸæ··è¿¹Tianchiç­‰ï¼Œå¸Œæœ›å’Œå¤§å®¶å¤šå¤šäº¤æµã€‚\n",
    ">         github: https://github.com/mlw67 ï¼ˆè¿‘æœŸä¼šåšä¸€äº›ä¹¦ç±æ¨å¯¼å’Œä»£ç çš„æ•´ç†ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datawhale é›¶åŸºç¡€å…¥é—¨æ•°æ®æŒ–æ˜-Task5 æ¨¡å‹èåˆ\n",
    "\n",
    "## äº”ã€æ¨¡å‹èåˆ\n",
    "\n",
    "Tip:æ­¤éƒ¨åˆ†ä¸ºé›¶åŸºç¡€å…¥é—¨æ•°æ®æŒ–æ˜çš„ Task5 æ¨¡å‹èåˆ éƒ¨åˆ†ï¼Œå¸¦ä½ æ¥äº†è§£å„ç§æ¨¡å‹ç»“æœçš„èåˆæ–¹å¼ï¼Œåœ¨æ¯”èµ›çš„æ”»åšæ—¶åˆ»å†²åˆºTopï¼Œæ¬¢è¿å¤§å®¶åç»­å¤šå¤šäº¤æµã€‚\n",
    "\n",
    "**èµ›é¢˜ï¼šé›¶åŸºç¡€å…¥é—¨æ•°æ®æŒ–æ˜ - äºŒæ‰‹è½¦äº¤æ˜“ä»·æ ¼é¢„æµ‹**\n",
    "\n",
    "åœ°å€ï¼šhttps://tianchi.aliyun.com/competition/entrance/231784/introduction?spm=5176.12281957.1004.1.38b02448ausjSX "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 æ¨¡å‹èåˆç›®æ ‡\n",
    "\n",
    "* å¯¹äºå¤šç§è°ƒå‚å®Œæˆçš„æ¨¡å‹è¿›è¡Œæ¨¡å‹èåˆã€‚\n",
    "\n",
    "* å®Œæˆå¯¹äºå¤šç§æ¨¡å‹çš„èåˆï¼Œæäº¤èåˆç»“æœå¹¶æ‰“å¡ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2  å†…å®¹ä»‹ç»\n",
    "\n",
    "æ¨¡å‹èåˆæ˜¯æ¯”èµ›åæœŸä¸€ä¸ªé‡è¦çš„ç¯èŠ‚ï¼Œå¤§ä½“æ¥è¯´æœ‰å¦‚ä¸‹çš„ç±»å‹æ–¹å¼ã€‚\n",
    "\n",
    "1.  ç®€å•åŠ æƒèåˆ:\n",
    "    - å›å½’ï¼ˆåˆ†ç±»æ¦‚ç‡ï¼‰ï¼šç®—æœ¯å¹³å‡èåˆï¼ˆArithmetic meanï¼‰ï¼Œå‡ ä½•å¹³å‡èåˆï¼ˆGeometric meanï¼‰ï¼›\n",
    "    - åˆ†ç±»ï¼šæŠ•ç¥¨ï¼ˆVoting)\n",
    "    - ç»¼åˆï¼šæ’åºèåˆ(Rank averaging)ï¼Œlogèåˆ\n",
    "\n",
    "\n",
    "2.  stacking/blending:\n",
    "    - æ„å»ºå¤šå±‚æ¨¡å‹ï¼Œå¹¶åˆ©ç”¨é¢„æµ‹ç»“æœå†æ‹Ÿåˆé¢„æµ‹ã€‚\n",
    "\n",
    "\n",
    "4.  boosting/baggingï¼ˆåœ¨xgboostï¼ŒAdaboost,GBDTä¸­å·²ç»ç”¨åˆ°ï¼‰:\n",
    "    - å¤šæ ‘çš„æå‡æ–¹æ³•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Stackingç›¸å…³ç†è®ºä»‹ç»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1)  ä»€ä¹ˆæ˜¯ stacking\n",
    "\n",
    "ç®€å•æ¥è¯´ stacking å°±æ˜¯å½“ç”¨åˆå§‹è®­ç»ƒæ•°æ®å­¦ä¹ å‡ºè‹¥å¹²ä¸ªåŸºå­¦ä¹ å™¨åï¼Œå°†è¿™å‡ ä¸ªå­¦ä¹ å™¨çš„é¢„æµ‹ç»“æœä½œä¸ºæ–°çš„è®­ç»ƒé›†ï¼Œæ¥å­¦ä¹ ä¸€ä¸ªæ–°çš„å­¦ä¹ å™¨ã€‚"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://jupter-oss.oss-cn-hangzhou.aliyuncs.com/public/files/image/2326541042/1584448793231_6TygjXwjNb.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å°†ä¸ªä½“å­¦ä¹ å™¨ç»“åˆåœ¨ä¸€èµ·çš„æ—¶å€™ä½¿ç”¨çš„æ–¹æ³•å«åšç»“åˆç­–ç•¥ã€‚å¯¹äºåˆ†ç±»é—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æŠ•ç¥¨æ³•æ¥é€‰æ‹©è¾“å‡ºæœ€å¤šçš„ç±»ã€‚å¯¹äºå›å½’é—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥å°†åˆ†ç±»å™¨è¾“å‡ºçš„ç»“æœæ±‚å¹³å‡å€¼ã€‚\n",
    "\n",
    "ä¸Šé¢è¯´çš„æŠ•ç¥¨æ³•å’Œå¹³å‡æ³•éƒ½æ˜¯å¾ˆæœ‰æ•ˆçš„ç»“åˆç­–ç•¥ï¼Œè¿˜æœ‰ä¸€ç§ç»“åˆç­–ç•¥æ˜¯ä½¿ç”¨å¦å¤–ä¸€ä¸ªæœºå™¨å­¦ä¹ ç®—æ³•æ¥å°†ä¸ªä½“æœºå™¨å­¦ä¹ å™¨çš„ç»“æœç»“åˆåœ¨ä¸€èµ·ï¼Œè¿™ä¸ªæ–¹æ³•å°±æ˜¯Stackingã€‚\n",
    "\n",
    "åœ¨stackingæ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬æŠŠä¸ªä½“å­¦ä¹ å™¨å«åšåˆçº§å­¦ä¹ å™¨ï¼Œç”¨äºç»“åˆçš„å­¦ä¹ å™¨å«åšæ¬¡çº§å­¦ä¹ å™¨æˆ–å…ƒå­¦ä¹ å™¨ï¼ˆmeta-learnerï¼‰ï¼Œæ¬¡çº§å­¦ä¹ å™¨ç”¨äºè®­ç»ƒçš„æ•°æ®å«åšæ¬¡çº§è®­ç»ƒé›†ã€‚æ¬¡çº§è®­ç»ƒé›†æ˜¯åœ¨è®­ç»ƒé›†ä¸Šç”¨åˆçº§å­¦ä¹ å™¨å¾—åˆ°çš„ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2)  å¦‚ä½•è¿›è¡Œ stacking\n",
    "ç®—æ³•ç¤ºæ„å›¾å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://jupter-oss.oss-cn-hangzhou.aliyuncs.com/public/files/image/2326541042/1584448806789_1ElRtHaacw.jpg)\n",
    "\n",
    "> å¼•ç”¨è‡ª è¥¿ç“œä¹¦ã€Šæœºå™¨å­¦ä¹ ã€‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* è¿‡ç¨‹1-3 æ˜¯è®­ç»ƒå‡ºæ¥ä¸ªä½“å­¦ä¹ å™¨ï¼Œä¹Ÿå°±æ˜¯åˆçº§å­¦ä¹ å™¨ã€‚\n",
    "* è¿‡ç¨‹5-9æ˜¯ ä½¿ç”¨è®­ç»ƒå‡ºæ¥çš„ä¸ªä½“å­¦ä¹ å™¨æ¥å¾—é¢„æµ‹çš„ç»“æœï¼Œè¿™ä¸ªé¢„æµ‹çš„ç»“æœå½“åšæ¬¡çº§å­¦ä¹ å™¨çš„è®­ç»ƒé›†ã€‚\n",
    "* è¿‡ç¨‹11 æ˜¯ç”¨åˆçº§å­¦ä¹ å™¨é¢„æµ‹çš„ç»“æœè®­ç»ƒå‡ºæ¬¡çº§å­¦ä¹ å™¨ï¼Œå¾—åˆ°æˆ‘ä»¬æœ€åè®­ç»ƒçš„æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 3ï¼‰Stackingçš„æ–¹æ³•è®²è§£\n",
    "\n",
    "é¦–å…ˆï¼Œæˆ‘ä»¬å…ˆä»ä¸€ç§â€œä¸é‚£ä¹ˆæ­£ç¡®â€ä½†æ˜¯å®¹æ˜“æ‡‚çš„Stackingæ–¹æ³•è®²èµ·ã€‚\n",
    "\n",
    "Stackingæ¨¡å‹æœ¬è´¨ä¸Šæ˜¯ä¸€ç§åˆ†å±‚çš„ç»“æ„ï¼Œè¿™é‡Œç®€å•èµ·è§ï¼Œåªåˆ†æäºŒçº§Stacking.å‡è®¾æˆ‘ä»¬æœ‰2ä¸ªåŸºæ¨¡å‹ Model1_1ã€Model1_2 å’Œ ä¸€ä¸ªæ¬¡çº§æ¨¡å‹Model2\n",
    "\n",
    "**Step 1.** åŸºæ¨¡å‹ Model1_1ï¼Œå¯¹è®­ç»ƒé›†trainè®­ç»ƒï¼Œç„¶åç”¨äºé¢„æµ‹ train å’Œ test çš„æ ‡ç­¾åˆ—ï¼Œåˆ†åˆ«æ˜¯P1ï¼ŒT1\n",
    "\n",
    "Model1_1 æ¨¡å‹è®­ç»ƒ:\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{c}{\\vdots} \\\\ {X_{train}} \\\\ {\\vdots}\\end{array}\\right) \\overbrace{\\Longrightarrow}^{\\text {Model1_1 Train} }\\left(\\begin{array}{c}{\\vdots} \\\\ {Y}_{True} \\\\ {\\vdots}\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "è®­ç»ƒåçš„æ¨¡å‹ Model1_1 åˆ†åˆ«åœ¨ train å’Œ test ä¸Šé¢„æµ‹ï¼Œå¾—åˆ°é¢„æµ‹æ ‡ç­¾åˆ†åˆ«æ˜¯P1ï¼ŒT1\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{c}{\\vdots} \\\\ {X_{train}} \\\\ {\\vdots}\\end{array}\\right) \\overbrace{\\Longrightarrow}^{\\text {Model1_1 Predict} }\\left(\\begin{array}{c}{\\vdots} \\\\ {P}_{1} \\\\ {\\vdots}\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{c}{\\vdots} \\\\ {X_{test}} \\\\ {\\vdots}\\end{array}\\right) \\overbrace{\\Longrightarrow}^{\\text {Model1_1 Predict} }\\left(\\begin{array}{c}{\\vdots} \\\\ {T_{1}} \\\\ {\\vdots}\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "**Step 2.** åŸºæ¨¡å‹ Model1_2 ï¼Œå¯¹è®­ç»ƒé›†trainè®­ç»ƒï¼Œç„¶åç”¨äºé¢„æµ‹trainå’Œtestçš„æ ‡ç­¾åˆ—ï¼Œåˆ†åˆ«æ˜¯P2ï¼ŒT2\n",
    "\n",
    "Model1_2 æ¨¡å‹è®­ç»ƒ:\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{c}{\\vdots} \\\\ {X_{train}} \\\\ {\\vdots}\\end{array}\\right) \\overbrace{\\Longrightarrow}^{\\text {Model1_2 Train} }\\left(\\begin{array}{c}{\\vdots} \\\\ {Y}_{True} \\\\ {\\vdots}\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "è®­ç»ƒåçš„æ¨¡å‹ Model1_2 åˆ†åˆ«åœ¨ train å’Œ test ä¸Šé¢„æµ‹ï¼Œå¾—åˆ°é¢„æµ‹æ ‡ç­¾åˆ†åˆ«æ˜¯P2ï¼ŒT2\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{c}{\\vdots} \\\\ {X_{train}} \\\\ {\\vdots}\\end{array}\\right) \\overbrace{\\Longrightarrow}^{\\text {Model1_2 Predict} }\\left(\\begin{array}{c}{\\vdots} \\\\ {P}_{2} \\\\ {\\vdots}\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{c}{\\vdots} \\\\ {X_{test}} \\\\ {\\vdots}\\end{array}\\right) \\overbrace{\\Longrightarrow}^{\\text {Model1_2 Predict} }\\left(\\begin{array}{c}{\\vdots} \\\\ {T_{2}} \\\\ {\\vdots}\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "**Step 3.** åˆ†åˆ«æŠŠP1,P2ä»¥åŠT1,T2åˆå¹¶ï¼Œå¾—åˆ°ä¸€ä¸ªæ–°çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†train2,test2.\n",
    "\n",
    "$$\n",
    "\\overbrace{\\left(\\begin{array}{c}{\\vdots} \\\\ {P_{1}} \\\\ {\\vdots}\\end{array} \\begin{array}{c}{\\vdots} \\\\ {P_{2}} \\\\ {\\vdots}\\end{array} \\right)}^{\\text {Train_2 }}  \n",
    "and \n",
    "\\overbrace{\\left(\\begin{array}{c}{\\vdots} \\\\ {T_{1}} \\\\ {\\vdots}\\end{array} \\begin{array}{c}{\\vdots} \\\\ {T_{2}} \\\\ {\\vdots}\\end{array} \\right)}^{\\text {Test_2 }}\n",
    "$$\n",
    "\n",
    "å†ç”¨ æ¬¡çº§æ¨¡å‹ Model2 ä»¥çœŸå®è®­ç»ƒé›†æ ‡ç­¾ä¸ºæ ‡ç­¾è®­ç»ƒ,ä»¥train2ä¸ºç‰¹å¾è¿›è¡Œè®­ç»ƒï¼Œé¢„æµ‹test2,å¾—åˆ°æœ€ç»ˆçš„æµ‹è¯•é›†é¢„æµ‹çš„æ ‡ç­¾åˆ— $Y_{Pre}$ã€‚\n",
    "\n",
    "$$\n",
    "\\overbrace{\\left(\\begin{array}{c}{\\vdots} \\\\ {P_{1}} \\\\ {\\vdots}\\end{array} \\begin{array}{c}{\\vdots} \\\\ {P_{2}} \\\\ {\\vdots}\\end{array} \\right)}^{\\text {Train_2 }} \\overbrace{\\Longrightarrow}^{\\text {Model2 Train} }\\left(\\begin{array}{c}{\\vdots} \\\\ {Y}_{True} \\\\ {\\vdots}\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\overbrace{\\left(\\begin{array}{c}{\\vdots} \\\\ {T_{1}} \\\\ {\\vdots}\\end{array} \\begin{array}{c}{\\vdots} \\\\ {T_{2}} \\\\ {\\vdots}\\end{array} \\right)}^{\\text {Test_2 }} \\overbrace{\\Longrightarrow}^{\\text {Model1_2 Predict} }\\left(\\begin{array}{c}{\\vdots} \\\\ {Y}_{Pre} \\\\ {\\vdots}\\end{array}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿™å°±æ˜¯æˆ‘ä»¬ä¸¤å±‚å †å çš„ä¸€ç§åŸºæœ¬çš„åŸå§‹æ€è·¯æƒ³æ³•ã€‚åœ¨ä¸åŒæ¨¡å‹é¢„æµ‹çš„ç»“æœåŸºç¡€ä¸Šå†åŠ ä¸€å±‚æ¨¡å‹ï¼Œè¿›è¡Œå†è®­ç»ƒï¼Œä»è€Œå¾—åˆ°æ¨¡å‹æœ€ç»ˆçš„é¢„æµ‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stackingæœ¬è´¨ä¸Šå°±æ˜¯è¿™ä¹ˆç›´æ¥çš„æ€è·¯ï¼Œä½†æ˜¯ç›´æ¥è¿™æ ·æœ‰æ—¶å¯¹äºå¦‚æœè®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆ†å¸ƒä¸é‚£ä¹ˆä¸€è‡´çš„æƒ…å†µä¸‹æ˜¯æœ‰ä¸€ç‚¹é—®é¢˜çš„ï¼Œå…¶é—®é¢˜åœ¨äºç”¨åˆå§‹æ¨¡å‹è®­ç»ƒçš„æ ‡ç­¾å†åˆ©ç”¨çœŸå®æ ‡ç­¾è¿›è¡Œå†è®­ç»ƒï¼Œæ¯«æ— ç–‘é—®ä¼šå¯¼è‡´ä¸€å®šçš„æ¨¡å‹è¿‡æ‹Ÿåˆè®­ç»ƒé›†ï¼Œè¿™æ ·æˆ–è®¸æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ³›åŒ–èƒ½åŠ›æˆ–è€…è¯´æ•ˆæœä¼šæœ‰ä¸€å®šçš„ä¸‹é™ï¼Œå› æ­¤ç°åœ¨çš„é—®é¢˜å˜æˆäº†å¦‚ä½•é™ä½å†è®­ç»ƒçš„è¿‡æ‹Ÿåˆæ€§ï¼Œè¿™é‡Œæˆ‘ä»¬ä¸€èˆ¬æœ‰ä¸¤ç§æ–¹æ³•ã€‚\n",
    "* 1. æ¬¡çº§æ¨¡å‹å°½é‡é€‰æ‹©ç®€å•çš„çº¿æ€§æ¨¡å‹\n",
    "* 2. åˆ©ç”¨KæŠ˜äº¤å‰éªŒè¯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-æŠ˜äº¤å‰éªŒè¯ï¼š\n",
    "è®­ç»ƒï¼š\n",
    "\n",
    "![](http://jupter-oss.oss-cn-hangzhou.aliyuncs.com/public/files/image/2326541042/1584448819632_YvJOXMk02P.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é¢„æµ‹ï¼š\n",
    "\n",
    "![](http://jupter-oss.oss-cn-hangzhou.aliyuncs.com/public/files/image/2326541042/1584448826203_k8KPy9n7D9.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 ä»£ç ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.1  å›å½’\\åˆ†ç±»æ¦‚ç‡-èåˆï¼š\n",
    "\n",
    "#### 1ï¼‰ç®€å•åŠ æƒå¹³å‡ï¼Œç»“æœç›´æ¥èåˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ç”Ÿæˆä¸€äº›ç®€å•çš„æ ·æœ¬æ•°æ®ï¼Œtest_prei ä»£è¡¨ç¬¬iä¸ªæ¨¡å‹çš„é¢„æµ‹å€¼\n",
    "test_pre1 = [1.2, 3.2, 2.1, 6.2]\n",
    "test_pre2 = [0.9, 3.1, 2.0, 5.9]\n",
    "test_pre3 = [1.1, 2.9, 2.2, 6.0]\n",
    "\n",
    "# y_test_true ä»£è¡¨ç¬¬æ¨¡å‹çš„çœŸå®å€¼\n",
    "y_test_true = [1, 3, 2, 6] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "## å®šä¹‰ç»“æœçš„åŠ æƒå¹³å‡å‡½æ•°\n",
    "def Weighted_method(test_pre1,test_pre2,test_pre3,w=[1/3,1/3,1/3]):\n",
    "    Weighted_result = w[0]*pd.Series(test_pre1)+w[1]*pd.Series(test_pre2)+w[2]*pd.Series(test_pre3)\n",
    "    return Weighted_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred1 MAE: 0.175\n",
      "Pred2 MAE: 0.075\n",
      "Pred3 MAE: 0.1\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "# å„æ¨¡å‹çš„é¢„æµ‹ç»“æœè®¡ç®—MAE\n",
    "print('Pred1 MAE:',metrics.mean_absolute_error(y_test_true, test_pre1))\n",
    "print('Pred2 MAE:',metrics.mean_absolute_error(y_test_true, test_pre2))\n",
    "print('Pred3 MAE:',metrics.mean_absolute_error(y_test_true, test_pre3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted_pre MAE: 0.0575\n"
     ]
    }
   ],
   "source": [
    "## æ ¹æ®åŠ æƒè®¡ç®—MAE\n",
    "w = [0.3,0.4,0.3] # å®šä¹‰æ¯”é‡æƒå€¼\n",
    "Weighted_pre = Weighted_method(test_pre1,test_pre2,test_pre3,w)\n",
    "print('Weighted_pre MAE:',metrics.mean_absolute_error(y_test_true, Weighted_pre))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¯ä»¥å‘ç°åŠ æƒç»“æœç›¸å¯¹äºä¹‹å‰çš„ç»“æœæ˜¯æœ‰æå‡çš„ï¼Œè¿™ç§æˆ‘ä»¬ç§°å…¶ä¸ºç®€å•çš„åŠ æƒå¹³å‡ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿˜æœ‰ä¸€äº›ç‰¹æ®Šçš„å½¢å¼ï¼Œæ¯”å¦‚meanå¹³å‡ï¼Œmedianå¹³å‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## å®šä¹‰ç»“æœçš„åŠ æƒå¹³å‡å‡½æ•°\n",
    "def Mean_method(test_pre1,test_pre2,test_pre3):\n",
    "    Mean_result = pd.concat([pd.Series(test_pre1),pd.Series(test_pre2),pd.Series(test_pre3)],axis=1).mean(axis=1)\n",
    "    return Mean_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean_pre MAE: 0.0666666666667\n"
     ]
    }
   ],
   "source": [
    "Mean_pre = Mean_method(test_pre1,test_pre2,test_pre3)\n",
    "print('Mean_pre MAE:',metrics.mean_absolute_error(y_test_true, Mean_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## å®šä¹‰ç»“æœçš„åŠ æƒå¹³å‡å‡½æ•°\n",
    "def Median_method(test_pre1,test_pre2,test_pre3):\n",
    "    Median_result = pd.concat([pd.Series(test_pre1),pd.Series(test_pre2),pd.Series(test_pre3)],axis=1).median(axis=1)\n",
    "    return Median_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median_pre MAE: 0.075\n"
     ]
    }
   ],
   "source": [
    "Median_pre = Median_method(test_pre1,test_pre2,test_pre3)\n",
    "print('Median_pre MAE:',metrics.mean_absolute_error(y_test_true, Median_pre))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2ï¼‰ Stackingèåˆ(å›å½’)ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "def Stacking_method(train_reg1,train_reg2,train_reg3,y_train_true,test_pre1,test_pre2,test_pre3,model_L2= linear_model.LinearRegression()):\n",
    "    model_L2.fit(pd.concat([pd.Series(train_reg1),pd.Series(train_reg2),pd.Series(train_reg3)],axis=1).values,y_train_true)\n",
    "    Stacking_result = model_L2.predict(pd.concat([pd.Series(test_pre1),pd.Series(test_pre2),pd.Series(test_pre3)],axis=1).values)\n",
    "    return Stacking_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ç”Ÿæˆä¸€äº›ç®€å•çš„æ ·æœ¬æ•°æ®ï¼Œtest_prei ä»£è¡¨ç¬¬iä¸ªæ¨¡å‹çš„é¢„æµ‹å€¼\n",
    "train_reg1 = [3.2, 8.2, 9.1, 5.2]\n",
    "train_reg2 = [2.9, 8.1, 9.0, 4.9]\n",
    "train_reg3 = [3.1, 7.9, 9.2, 5.0]\n",
    "# y_test_true ä»£è¡¨ç¬¬æ¨¡å‹çš„çœŸå®å€¼\n",
    "y_train_true = [3, 8, 9, 5] \n",
    "\n",
    "test_pre1 = [1.2, 3.2, 2.1, 6.2]\n",
    "test_pre2 = [0.9, 3.1, 2.0, 5.9]\n",
    "test_pre3 = [1.1, 2.9, 2.2, 6.0]\n",
    "\n",
    "# y_test_true ä»£è¡¨ç¬¬æ¨¡å‹çš„çœŸå®å€¼\n",
    "y_test_true = [1, 3, 2, 6] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking_pre MAE: 0.0421348314607\n"
     ]
    }
   ],
   "source": [
    "model_L2= linear_model.LinearRegression()\n",
    "Stacking_pre = Stacking_method(train_reg1,train_reg2,train_reg3,y_train_true,\n",
    "                               test_pre1,test_pre2,test_pre3,model_L2)\n",
    "print('Stacking_pre MAE:',metrics.mean_absolute_error(y_test_true, Stacking_pre))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¯ä»¥å‘ç°æ¨¡å‹ç»“æœç›¸å¯¹äºä¹‹å‰æœ‰è¿›ä¸€æ­¥çš„æå‡ï¼Œè¿™æ˜¯æˆ‘ä»¬éœ€è¦æ³¨æ„çš„ä¸€ç‚¹æ˜¯ï¼Œå¯¹äºç¬¬äºŒå±‚Stackingçš„æ¨¡å‹ä¸å®œé€‰å–çš„è¿‡äºå¤æ‚ï¼Œè¿™æ ·ä¼šå¯¼è‡´æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šè¿‡æ‹Ÿåˆï¼Œä»è€Œä½¿å¾—åœ¨æµ‹è¯•é›†ä¸Šå¹¶ä¸èƒ½è¾¾åˆ°å¾ˆå¥½çš„æ•ˆæœã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.2 åˆ†ç±»æ¨¡å‹èåˆï¼š\n",
    "å¯¹äºåˆ†ç±»ï¼ŒåŒæ ·çš„å¯ä»¥ä½¿ç”¨èåˆæ–¹æ³•ï¼Œæ¯”å¦‚ç®€å•æŠ•ç¥¨ï¼ŒStacking..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1ï¼‰VotingæŠ•ç¥¨æœºåˆ¶ï¼š\n",
    "\n",
    "Votingå³æŠ•ç¥¨æœºåˆ¶ï¼Œåˆ†ä¸ºè½¯æŠ•ç¥¨å’Œç¡¬æŠ•ç¥¨ä¸¤ç§ï¼Œå…¶åŸç†é‡‡ç”¨å°‘æ•°æœä»å¤šæ•°çš„æ€æƒ³ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.97 (+/- 0.02) [XGBBoosting]\n",
      "Accuracy: 0.33 (+/- 0.00) [Random Forest]\n",
      "Accuracy: 0.95 (+/- 0.03) [SVM]\n",
      "Accuracy: 0.94 (+/- 0.04) [Ensemble]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "ç¡¬æŠ•ç¥¨ï¼šå¯¹å¤šä¸ªæ¨¡å‹ç›´æ¥è¿›è¡ŒæŠ•ç¥¨ï¼Œä¸åŒºåˆ†æ¨¡å‹ç»“æœçš„ç›¸å¯¹é‡è¦åº¦ï¼Œæœ€ç»ˆæŠ•ç¥¨æ•°æœ€å¤šçš„ç±»ä¸ºæœ€ç»ˆè¢«é¢„æµ‹çš„ç±»ã€‚\n",
    "'''\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "x=iris.data\n",
    "y=iris.target\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3)\n",
    "\n",
    "clf1 = XGBClassifier(learning_rate=0.1, n_estimators=150, max_depth=3, min_child_weight=2, subsample=0.7,\n",
    "                     colsample_bytree=0.6, objective='binary:logistic')\n",
    "clf2 = RandomForestClassifier(n_estimators=50, max_depth=1, min_samples_split=4,\n",
    "                              min_samples_leaf=63,oob_score=True)\n",
    "clf3 = SVC(C=0.1)\n",
    "\n",
    "# ç¡¬æŠ•ç¥¨\n",
    "eclf = VotingClassifier(estimators=[('xgb', clf1), ('rf', clf2), ('svc', clf3)], voting='hard')\n",
    "for clf, label in zip([clf1, clf2, clf3, eclf], ['XGBBoosting', 'Random Forest', 'SVM', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, x, y, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96 (+/- 0.02) [XGBBoosting]\n",
      "Accuracy: 0.33 (+/- 0.00) [Random Forest]\n",
      "Accuracy: 0.95 (+/- 0.03) [SVM]\n",
      "Accuracy: 0.96 (+/- 0.02) [Ensemble]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "è½¯æŠ•ç¥¨ï¼šå’Œç¡¬æŠ•ç¥¨åŸç†ç›¸åŒï¼Œå¢åŠ äº†è®¾ç½®æƒé‡çš„åŠŸèƒ½ï¼Œå¯ä»¥ä¸ºä¸åŒæ¨¡å‹è®¾ç½®ä¸åŒæƒé‡ï¼Œè¿›è€ŒåŒºåˆ«æ¨¡å‹ä¸åŒçš„é‡è¦åº¦ã€‚\n",
    "'''\n",
    "x=iris.data\n",
    "y=iris.target\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3)\n",
    "\n",
    "clf1 = XGBClassifier(learning_rate=0.1, n_estimators=150, max_depth=3, min_child_weight=2, subsample=0.8,\n",
    "                     colsample_bytree=0.8, objective='binary:logistic')\n",
    "clf2 = RandomForestClassifier(n_estimators=50, max_depth=1, min_samples_split=4,\n",
    "                              min_samples_leaf=63,oob_score=True)\n",
    "clf3 = SVC(C=0.1, probability=True)\n",
    "\n",
    "# è½¯æŠ•ç¥¨\n",
    "eclf = VotingClassifier(estimators=[('xgb', clf1), ('rf', clf2), ('svc', clf3)], voting='soft', weights=[2, 1, 1])\n",
    "clf1.fit(x_train, y_train)\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, eclf], ['XGBBoosting', 'Random Forest', 'SVM', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, x, y, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2ï¼‰åˆ†ç±»çš„Stacking\\Blendingèåˆï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stackingæ˜¯ä¸€ç§åˆ†å±‚æ¨¡å‹é›†æˆæ¡†æ¶ã€‚\n",
    "\n",
    "> ä»¥ä¸¤å±‚ä¸ºä¾‹ï¼Œç¬¬ä¸€å±‚ç”±å¤šä¸ªåŸºå­¦ä¹ å™¨ç»„æˆï¼Œå…¶è¾“å…¥ä¸ºåŸå§‹è®­ç»ƒé›†ï¼Œç¬¬äºŒå±‚çš„æ¨¡å‹åˆ™æ˜¯ä»¥ç¬¬ä¸€å±‚åŸºå­¦ä¹ å™¨çš„è¾“å‡ºä½œä¸ºè®­ç»ƒé›†è¿›è¡Œå†è®­ç»ƒï¼Œä»è€Œå¾—åˆ°å®Œæ•´çš„stackingæ¨¡å‹, stackingä¸¤å±‚æ¨¡å‹éƒ½ä½¿ç”¨äº†å…¨éƒ¨çš„è®­ç»ƒæ•°æ®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val auc Score: 1.000000\n",
      "val auc Score: 0.500000\n",
      "val auc Score: 0.500000\n",
      "val auc Score: 0.500000\n",
      "val auc Score: 0.500000\n",
      "Val auc Score of Stacking: 1.000000\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "5-Fold Stacking\n",
    "'''\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier,GradientBoostingClassifier\n",
    "import pandas as pd\n",
    "#åˆ›å»ºè®­ç»ƒçš„æ•°æ®é›†\n",
    "data_0 = iris.data\n",
    "data = data_0[:100,:]\n",
    "\n",
    "target_0 = iris.target\n",
    "target = target_0[:100]\n",
    "\n",
    "#æ¨¡å‹èåˆä¸­ä½¿ç”¨åˆ°çš„å„ä¸ªå•æ¨¡å‹\n",
    "clfs = [LogisticRegression(solver='lbfgs'),\n",
    "        RandomForestClassifier(n_estimators=5, n_jobs=-1, criterion='gini'),\n",
    "        ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='gini'),\n",
    "        ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='entropy'),\n",
    "        GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=5)]\n",
    " \n",
    "#åˆ‡åˆ†ä¸€éƒ¨åˆ†æ•°æ®ä½œä¸ºæµ‹è¯•é›†\n",
    "X, X_predict, y, y_predict = train_test_split(data, target, test_size=0.3, random_state=2020)\n",
    "\n",
    "dataset_blend_train = np.zeros((X.shape[0], len(clfs)))\n",
    "dataset_blend_test = np.zeros((X_predict.shape[0], len(clfs)))\n",
    "\n",
    "#5æŠ˜stacking\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits)\n",
    "skf = skf.split(X, y)\n",
    "\n",
    "for j, clf in enumerate(clfs):\n",
    "    #ä¾æ¬¡è®­ç»ƒå„ä¸ªå•æ¨¡å‹\n",
    "    dataset_blend_test_j = np.zeros((X_predict.shape[0], 5))\n",
    "    for i, (train, test) in enumerate(skf):\n",
    "        #5-Foldäº¤å‰è®­ç»ƒï¼Œä½¿ç”¨ç¬¬iä¸ªéƒ¨åˆ†ä½œä¸ºé¢„æµ‹ï¼Œå‰©ä½™çš„éƒ¨åˆ†æ¥è®­ç»ƒæ¨¡å‹ï¼Œè·å¾—å…¶é¢„æµ‹çš„è¾“å‡ºä½œä¸ºç¬¬iéƒ¨åˆ†çš„æ–°ç‰¹å¾ã€‚\n",
    "        X_train, y_train, X_test, y_test = X[train], y[train], X[test], y[test]\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_submission = clf.predict_proba(X_test)[:, 1]\n",
    "        dataset_blend_train[test, j] = y_submission\n",
    "        dataset_blend_test_j[:, i] = clf.predict_proba(X_predict)[:, 1]\n",
    "    #å¯¹äºæµ‹è¯•é›†ï¼Œç›´æ¥ç”¨è¿™kä¸ªæ¨¡å‹çš„é¢„æµ‹å€¼å‡å€¼ä½œä¸ºæ–°çš„ç‰¹å¾ã€‚\n",
    "    dataset_blend_test[:, j] = dataset_blend_test_j.mean(1)\n",
    "    print(\"val auc Score: %f\" % roc_auc_score(y_predict, dataset_blend_test[:, j]))\n",
    "\n",
    "clf = LogisticRegression(solver='lbfgs')\n",
    "clf.fit(dataset_blend_train, y)\n",
    "y_submission = clf.predict_proba(dataset_blend_test)[:, 1]\n",
    "\n",
    "print(\"Val auc Score of Stacking: %f\" % (roc_auc_score(y_predict, y_submission)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blendingï¼Œå…¶å®å’ŒStackingæ˜¯ä¸€ç§ç±»ä¼¼çš„å¤šå±‚æ¨¡å‹èåˆçš„å½¢å¼\n",
    "\n",
    "> å…¶ä¸»è¦æ€è·¯æ˜¯æŠŠåŸå§‹çš„è®­ç»ƒé›†å…ˆåˆ†æˆä¸¤éƒ¨åˆ†ï¼Œæ¯”å¦‚70%çš„æ•°æ®ä½œä¸ºæ–°çš„è®­ç»ƒé›†ï¼Œå‰©ä¸‹30%çš„æ•°æ®ä½œä¸ºæµ‹è¯•é›†ã€‚\n",
    "\n",
    "> åœ¨ç¬¬ä¸€å±‚ï¼Œæˆ‘ä»¬åœ¨è¿™70%çš„æ•°æ®ä¸Šè®­ç»ƒå¤šä¸ªæ¨¡å‹ï¼Œç„¶åå»é¢„æµ‹é‚£30%æ•°æ®çš„labelï¼ŒåŒæ—¶ä¹Ÿé¢„æµ‹testé›†çš„labelã€‚\n",
    "\n",
    "> åœ¨ç¬¬äºŒå±‚ï¼Œæˆ‘ä»¬å°±ç›´æ¥ç”¨è¿™30%æ•°æ®åœ¨ç¬¬ä¸€å±‚é¢„æµ‹çš„ç»“æœåšä¸ºæ–°ç‰¹å¾ç»§ç»­è®­ç»ƒï¼Œç„¶åç”¨testé›†ç¬¬ä¸€å±‚é¢„æµ‹çš„labelåšç‰¹å¾ï¼Œç”¨ç¬¬äºŒå±‚è®­ç»ƒçš„æ¨¡å‹åšè¿›ä¸€æ­¥é¢„æµ‹\n",
    "\n",
    "å…¶ä¼˜ç‚¹åœ¨äºï¼š\n",
    "* 1.æ¯”stackingç®€å•ï¼ˆå› ä¸ºä¸ç”¨è¿›è¡Œkæ¬¡çš„äº¤å‰éªŒè¯æ¥è·å¾—stacker featureï¼‰\n",
    "* 2.é¿å¼€äº†ä¸€ä¸ªä¿¡æ¯æ³„éœ²é—®é¢˜ï¼šgenerlizerså’Œstackerä½¿ç”¨äº†ä¸ä¸€æ ·çš„æ•°æ®é›†\n",
    "\n",
    "ç¼ºç‚¹åœ¨äºï¼š\n",
    "* 1.ä½¿ç”¨äº†å¾ˆå°‘çš„æ•°æ®ï¼ˆç¬¬äºŒé˜¶æ®µçš„blenderåªä½¿ç”¨training set10%çš„é‡ï¼‰\n",
    "* 2.blenderå¯èƒ½ä¼šè¿‡æ‹Ÿåˆ\n",
    "* 3.stackingä½¿ç”¨å¤šæ¬¡çš„äº¤å‰éªŒè¯ä¼šæ¯”è¾ƒç¨³å¥\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val auc Score: 1.000000\n",
      "val auc Score: 1.000000\n",
      "val auc Score: 1.000000\n",
      "val auc Score: 1.000000\n",
      "val auc Score: 1.000000\n",
      "Val auc Score of Blending: 1.000000\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Blending\n",
    "'''\n",
    " \n",
    "#åˆ›å»ºè®­ç»ƒçš„æ•°æ®é›†\n",
    "#åˆ›å»ºè®­ç»ƒçš„æ•°æ®é›†\n",
    "data_0 = iris.data\n",
    "data = data_0[:100,:]\n",
    "\n",
    "target_0 = iris.target\n",
    "target = target_0[:100]\n",
    " \n",
    "#æ¨¡å‹èåˆä¸­ä½¿ç”¨åˆ°çš„å„ä¸ªå•æ¨¡å‹\n",
    "clfs = [LogisticRegression(solver='lbfgs'),\n",
    "        RandomForestClassifier(n_estimators=5, n_jobs=-1, criterion='gini'),\n",
    "        RandomForestClassifier(n_estimators=5, n_jobs=-1, criterion='entropy'),\n",
    "        ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='gini'),\n",
    "        #ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='entropy'),\n",
    "        GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=5)]\n",
    " \n",
    "#åˆ‡åˆ†ä¸€éƒ¨åˆ†æ•°æ®ä½œä¸ºæµ‹è¯•é›†\n",
    "X, X_predict, y, y_predict = train_test_split(data, target, test_size=0.3, random_state=2020)\n",
    "\n",
    "#åˆ‡åˆ†è®­ç»ƒæ•°æ®é›†ä¸ºd1,d2ä¸¤éƒ¨åˆ†\n",
    "X_d1, X_d2, y_d1, y_d2 = train_test_split(X, y, test_size=0.5, random_state=2020)\n",
    "dataset_d1 = np.zeros((X_d2.shape[0], len(clfs)))\n",
    "dataset_d2 = np.zeros((X_predict.shape[0], len(clfs)))\n",
    " \n",
    "for j, clf in enumerate(clfs):\n",
    "    #ä¾æ¬¡è®­ç»ƒå„ä¸ªå•æ¨¡å‹\n",
    "    clf.fit(X_d1, y_d1)\n",
    "    y_submission = clf.predict_proba(X_d2)[:, 1]\n",
    "    dataset_d1[:, j] = y_submission\n",
    "    #å¯¹äºæµ‹è¯•é›†ï¼Œç›´æ¥ç”¨è¿™kä¸ªæ¨¡å‹çš„é¢„æµ‹å€¼ä½œä¸ºæ–°çš„ç‰¹å¾ã€‚\n",
    "    dataset_d2[:, j] = clf.predict_proba(X_predict)[:, 1]\n",
    "    print(\"val auc Score: %f\" % roc_auc_score(y_predict, dataset_d2[:, j]))\n",
    "\n",
    "#èåˆä½¿ç”¨çš„æ¨¡å‹\n",
    "clf = GradientBoostingClassifier(learning_rate=0.02, subsample=0.5, max_depth=6, n_estimators=30)\n",
    "clf.fit(dataset_d1, y_d2)\n",
    "y_submission = clf.predict_proba(dataset_d2)[:, 1]\n",
    "print(\"Val auc Score of Blending: %f\" % (roc_auc_score(y_predict, y_submission)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å‚è€ƒåšå®¢ï¼šhttps://blog.csdn.net/Noob_daniel/article/details/76087829"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3ï¼‰åˆ†ç±»çš„Stackingèåˆ(åˆ©ç”¨mlxtend)ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mlxtend\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import itertools\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from mlxtend.plotting import plot_learning_curves\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "# ä»¥pythonè‡ªå¸¦çš„é¸¢å°¾èŠ±æ•°æ®é›†ä¸ºä¾‹\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:, 1:3], iris.target\n",
    "\n",
    "clf1 = KNeighborsClassifier(n_neighbors=1)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "lr = LogisticRegression()\n",
    "sclf = StackingClassifier(classifiers=[clf1, clf2, clf3], \n",
    "                          meta_classifier=lr)\n",
    "\n",
    "label = ['KNN', 'Random Forest', 'Naive Bayes', 'Stacking Classifier']\n",
    "clf_list = [clf1, clf2, clf3, sclf]\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "grid = itertools.product([0,1],repeat=2)\n",
    "\n",
    "clf_cv_mean = []\n",
    "clf_cv_std = []\n",
    "for clf, label, grd in zip(clf_list, label, grid):\n",
    "        \n",
    "    scores = cross_val_score(clf, X, y, cv=3, scoring='accuracy')\n",
    "    print(\"Accuracy: %.2f (+/- %.2f) [%s]\" %(scores.mean(), scores.std(), label))\n",
    "    clf_cv_mean.append(scores.mean())\n",
    "    clf_cv_std.append(scores.std())\n",
    "        \n",
    "    clf.fit(X, y)\n",
    "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "    fig = plot_decision_regions(X=X, y=y, clf=clf)\n",
    "    plt.title(label)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¯ä»¥å‘ç° åŸºæ¨¡å‹ ç”¨ 'KNN', 'Random Forest', 'Naive Bayes' ç„¶åå†è¿™åŸºç¡€ä¸Š æ¬¡çº§æ¨¡å‹åŠ ä¸€ä¸ª 'LogisticRegression'ï¼Œæ¨¡å‹æµ‹è¯•æ•ˆæœæœ‰ç€å¾ˆå¥½çš„æå‡ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.3 ä¸€äº›å…¶å®ƒæ–¹æ³•ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å°†ç‰¹å¾æ”¾è¿›æ¨¡å‹ä¸­é¢„æµ‹ï¼Œå¹¶å°†é¢„æµ‹ç»“æœå˜æ¢å¹¶ä½œä¸ºæ–°çš„ç‰¹å¾åŠ å…¥åŸæœ‰ç‰¹å¾ä¸­å†ç»è¿‡æ¨¡å‹é¢„æµ‹ç»“æœ ï¼ˆStackingå˜åŒ–ï¼‰\n",
    "\n",
    "ï¼ˆå¯ä»¥åå¤é¢„æµ‹å¤šæ¬¡å°†ç»“æœåŠ å…¥æœ€åçš„ç‰¹å¾ä¸­ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T12:50:55.020619Z",
     "start_time": "2019-12-24T12:50:55.004659Z"
    }
   },
   "outputs": [],
   "source": [
    "def Ensemble_add_feature(train,test,target,clfs):\n",
    "    \n",
    "    # n_flods = 5\n",
    "    # skf = list(StratifiedKFold(y, n_folds=n_flods))\n",
    "\n",
    "    train_ = np.zeros((train.shape[0],len(clfs*2)))\n",
    "    test_ = np.zeros((test.shape[0],len(clfs*2)))\n",
    "\n",
    "    for j,clf in enumerate(clfs):\n",
    "        '''ä¾æ¬¡è®­ç»ƒå„ä¸ªå•æ¨¡å‹'''\n",
    "        # print(j, clf)\n",
    "        '''ä½¿ç”¨ç¬¬1ä¸ªéƒ¨åˆ†ä½œä¸ºé¢„æµ‹ï¼Œç¬¬2éƒ¨åˆ†æ¥è®­ç»ƒæ¨¡å‹ï¼Œè·å¾—å…¶é¢„æµ‹çš„è¾“å‡ºä½œä¸ºç¬¬2éƒ¨åˆ†çš„æ–°ç‰¹å¾ã€‚'''\n",
    "        # X_train, y_train, X_test, y_test = X[train], y[train], X[test], y[test]\n",
    "\n",
    "        clf.fit(train,target)\n",
    "        y_train = clf.predict(train)\n",
    "        y_test = clf.predict(test)\n",
    "\n",
    "        ## æ–°ç‰¹å¾ç”Ÿæˆ\n",
    "        train_[:,j*2] = y_train**2\n",
    "        test_[:,j*2] = y_test**2\n",
    "        train_[:, j+1] = np.exp(y_train)\n",
    "        test_[:, j+1] = np.exp(y_test)\n",
    "        # print(\"val auc Score: %f\" % r2_score(y_predict, dataset_d2[:, j]))\n",
    "        print('Method ',j)\n",
    "    \n",
    "    train_ = pd.DataFrame(train_)\n",
    "    test_ = pd.DataFrame(test_)\n",
    "    return train_,test_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method  0\n",
      "Method  1\n",
      "Method  2\n",
      "Method  3\n",
      "Method  4\n",
      "Val auc Score of stacking: 1.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "\n",
    "data_0 = iris.data\n",
    "data = data_0[:100,:]\n",
    "\n",
    "target_0 = iris.target\n",
    "target = target_0[:100]\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(data,target,test_size=0.3)\n",
    "x_train = pd.DataFrame(x_train) ; x_test = pd.DataFrame(x_test)\n",
    "\n",
    "#æ¨¡å‹èåˆä¸­ä½¿ç”¨åˆ°çš„å„ä¸ªå•æ¨¡å‹\n",
    "clfs = [LogisticRegression(),\n",
    "        RandomForestClassifier(n_estimators=5, n_jobs=-1, criterion='gini'),\n",
    "        ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='gini'),\n",
    "        ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='entropy'),\n",
    "        GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=5)]\n",
    "\n",
    "New_train,New_test = Ensemble_add_feature(x_train,x_test,y_train,clfs)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "# clf = GradientBoostingClassifier(learning_rate=0.02, subsample=0.5, max_depth=6, n_estimators=30)\n",
    "clf.fit(New_train, y_train)\n",
    "y_emb = clf.predict_proba(New_test)[:, 1]\n",
    "\n",
    "print(\"Val auc Score of stacking: %f\" % (roc_auc_score(y_test, y_emb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4.4 æœ¬èµ›é¢˜ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "import itertools\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "# from mlxtend.plotting import plot_learning_curves\n",
    "# from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.decomposition import PCA,FastICA,FactorAnalysis,SparsePCA\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV,cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150000, 31)\n",
      "(50000, 30)\n"
     ]
    }
   ],
   "source": [
    "## æ•°æ®è¯»å–\n",
    "Train_data = pd.read_csv('datalab/231784/used_car_train_20200313.csv', sep=' ')\n",
    "TestA_data = pd.read_csv('datalab/231784/used_car_testA_20200313.csv', sep=' ')\n",
    "\n",
    "print(Train_data.shape)\n",
    "print(TestA_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SaleID</th>\n",
       "      <th>name</th>\n",
       "      <th>regDate</th>\n",
       "      <th>model</th>\n",
       "      <th>brand</th>\n",
       "      <th>bodyType</th>\n",
       "      <th>fuelType</th>\n",
       "      <th>gearbox</th>\n",
       "      <th>power</th>\n",
       "      <th>kilometer</th>\n",
       "      <th>...</th>\n",
       "      <th>v_5</th>\n",
       "      <th>v_6</th>\n",
       "      <th>v_7</th>\n",
       "      <th>v_8</th>\n",
       "      <th>v_9</th>\n",
       "      <th>v_10</th>\n",
       "      <th>v_11</th>\n",
       "      <th>v_12</th>\n",
       "      <th>v_13</th>\n",
       "      <th>v_14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>736</td>\n",
       "      <td>20040402</td>\n",
       "      <td>30.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60</td>\n",
       "      <td>12.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.235676</td>\n",
       "      <td>0.101988</td>\n",
       "      <td>0.129549</td>\n",
       "      <td>0.022816</td>\n",
       "      <td>0.097462</td>\n",
       "      <td>-2.881803</td>\n",
       "      <td>2.804097</td>\n",
       "      <td>-2.420821</td>\n",
       "      <td>0.795292</td>\n",
       "      <td>0.914762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2262</td>\n",
       "      <td>20030301</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.264777</td>\n",
       "      <td>0.121004</td>\n",
       "      <td>0.135731</td>\n",
       "      <td>0.026597</td>\n",
       "      <td>0.020582</td>\n",
       "      <td>-4.900482</td>\n",
       "      <td>2.096338</td>\n",
       "      <td>-1.030483</td>\n",
       "      <td>-1.722674</td>\n",
       "      <td>0.245522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>14874</td>\n",
       "      <td>20040403</td>\n",
       "      <td>115.0</td>\n",
       "      <td>15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>163</td>\n",
       "      <td>12.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251410</td>\n",
       "      <td>0.114912</td>\n",
       "      <td>0.165147</td>\n",
       "      <td>0.062173</td>\n",
       "      <td>0.027075</td>\n",
       "      <td>-4.846749</td>\n",
       "      <td>1.803559</td>\n",
       "      <td>1.565330</td>\n",
       "      <td>-0.832687</td>\n",
       "      <td>-0.229963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>71865</td>\n",
       "      <td>19960908</td>\n",
       "      <td>109.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>193</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.274293</td>\n",
       "      <td>0.110300</td>\n",
       "      <td>0.121964</td>\n",
       "      <td>0.033395</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.509599</td>\n",
       "      <td>1.285940</td>\n",
       "      <td>-0.501868</td>\n",
       "      <td>-2.438353</td>\n",
       "      <td>-0.478699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>111080</td>\n",
       "      <td>20120103</td>\n",
       "      <td>110.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228036</td>\n",
       "      <td>0.073205</td>\n",
       "      <td>0.091880</td>\n",
       "      <td>0.078819</td>\n",
       "      <td>0.121534</td>\n",
       "      <td>-1.896240</td>\n",
       "      <td>0.910783</td>\n",
       "      <td>0.931110</td>\n",
       "      <td>2.834518</td>\n",
       "      <td>1.923482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SaleID    name   regDate  model  brand  bodyType  fuelType  gearbox  power  \\\n",
       "0       0     736  20040402   30.0      6       1.0       0.0      0.0     60   \n",
       "1       1    2262  20030301   40.0      1       2.0       0.0      0.0      0   \n",
       "2       2   14874  20040403  115.0     15       1.0       0.0      0.0    163   \n",
       "3       3   71865  19960908  109.0     10       0.0       0.0      1.0    193   \n",
       "4       4  111080  20120103  110.0      5       1.0       0.0      0.0     68   \n",
       "\n",
       "   kilometer    ...          v_5       v_6       v_7       v_8       v_9  \\\n",
       "0       12.5    ...     0.235676  0.101988  0.129549  0.022816  0.097462   \n",
       "1       15.0    ...     0.264777  0.121004  0.135731  0.026597  0.020582   \n",
       "2       12.5    ...     0.251410  0.114912  0.165147  0.062173  0.027075   \n",
       "3       15.0    ...     0.274293  0.110300  0.121964  0.033395  0.000000   \n",
       "4        5.0    ...     0.228036  0.073205  0.091880  0.078819  0.121534   \n",
       "\n",
       "       v_10      v_11      v_12      v_13      v_14  \n",
       "0 -2.881803  2.804097 -2.420821  0.795292  0.914762  \n",
       "1 -4.900482  2.096338 -1.030483 -1.722674  0.245522  \n",
       "2 -4.846749  1.803559  1.565330 -0.832687 -0.229963  \n",
       "3 -4.509599  1.285940 -0.501868 -2.438353 -0.478699  \n",
       "4 -1.896240  0.910783  0.931110  2.834518  1.923482  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['SaleID', 'name', 'regDate', 'model', 'brand', 'bodyType', 'fuelType',\n",
      "       'gearbox', 'power', 'kilometer', 'regionCode', 'seller', 'offerType',\n",
      "       'creatDate', 'price', 'v_0', 'v_1', 'v_2', 'v_3', 'v_4', 'v_5', 'v_6',\n",
      "       'v_7', 'v_8', 'v_9', 'v_10', 'v_11', 'v_12', 'v_13', 'v_14'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "numerical_cols = Train_data.select_dtypes(exclude = 'object').columns\n",
    "print(numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [col for col in numerical_cols if col not in ['SaleID','name','regDate','price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train shape: (150000, 26)\n",
      "X test shape: (50000, 26)\n"
     ]
    }
   ],
   "source": [
    "X_data = Train_data[feature_cols]\n",
    "Y_data = Train_data['price']\n",
    "\n",
    "X_test  = TestA_data[feature_cols]\n",
    "\n",
    "print('X train shape:',X_data.shape)\n",
    "print('X test shape:',X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sta_inf(data):\n",
    "    print('_min',np.min(data))\n",
    "    print('_max:',np.max(data))\n",
    "    print('_mean',np.mean(data))\n",
    "    print('_ptp',np.ptp(data))\n",
    "    print('_std',np.std(data))\n",
    "    print('_var',np.var(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sta of label:\n",
      "_min 11\n",
      "_max: 99999\n",
      "_mean 5923.32733333\n",
      "_ptp 99988\n",
      "_std 7501.97346988\n",
      "_var 56279605.9427\n"
     ]
    }
   ],
   "source": [
    "print('Sta of label:')\n",
    "Sta_inf(Y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = X_data.fillna(-1)\n",
    "X_test = X_test.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_lr(x_train,y_train):\n",
    "    reg_model = linear_model.LinearRegression()\n",
    "    reg_model.fit(x_train,y_train)\n",
    "    return reg_model\n",
    "\n",
    "def build_model_ridge(x_train,y_train):\n",
    "    reg_model = linear_model.Ridge(alpha=0.8)#alphas=range(1,100,5)\n",
    "    reg_model.fit(x_train,y_train)\n",
    "    return reg_model\n",
    "\n",
    "def build_model_lasso(x_train,y_train):\n",
    "    reg_model = linear_model.LassoCV()\n",
    "    reg_model.fit(x_train,y_train)\n",
    "    return reg_model\n",
    "\n",
    "def build_model_gbdt(x_train,y_train):\n",
    "    estimator =GradientBoostingRegressor(loss='ls',subsample= 0.85,max_depth= 5,n_estimators = 100)\n",
    "    param_grid = { \n",
    "            'learning_rate': [0.05,0.08,0.1,0.2],\n",
    "            }\n",
    "    gbdt = GridSearchCV(estimator, param_grid,cv=3)\n",
    "    gbdt.fit(x_train,y_train)\n",
    "    print(gbdt.best_params_)\n",
    "    # print(gbdt.best_estimator_ )\n",
    "    return gbdt\n",
    "\n",
    "def build_model_xgb(x_train,y_train):\n",
    "    model = xgb.XGBRegressor(n_estimators=120, learning_rate=0.08, gamma=0, subsample=0.8,\\\n",
    "        colsample_bytree=0.9, max_depth=5) #, objective ='reg:squarederror'\n",
    "    model.fit(x_train, y_train)\n",
    "    return model\n",
    "\n",
    "def build_model_lgb(x_train,y_train):\n",
    "    estimator = lgb.LGBMRegressor(num_leaves=63,n_estimators = 100)\n",
    "    param_grid = {\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "    }\n",
    "    gbm = GridSearchCV(estimator, param_grid)\n",
    "    gbm.fit(x_train, y_train)\n",
    "    return gbm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2ï¼‰XGBoostçš„äº”æŠ˜äº¤å‰å›å½’éªŒè¯å®ç°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train mae: 558.212360169\n",
      "Val mae 693.120168439\n"
     ]
    }
   ],
   "source": [
    "## xgb\n",
    "xgr = xgb.XGBRegressor(n_estimators=120, learning_rate=0.1, subsample=0.8,\\\n",
    "        colsample_bytree=0.9, max_depth=7) # ,objective ='reg:squarederror'\n",
    "\n",
    "scores_train = []\n",
    "scores = []\n",
    "\n",
    "## 5æŠ˜äº¤å‰éªŒè¯æ–¹å¼\n",
    "sk=StratifiedKFold(n_splits=5,shuffle=True,random_state=0)\n",
    "for train_ind,val_ind in sk.split(X_data,Y_data):\n",
    "    \n",
    "    train_x=X_data.iloc[train_ind].values\n",
    "    train_y=Y_data.iloc[train_ind]\n",
    "    val_x=X_data.iloc[val_ind].values\n",
    "    val_y=Y_data.iloc[val_ind]\n",
    "    \n",
    "    xgr.fit(train_x,train_y)\n",
    "    pred_train_xgb=xgr.predict(train_x)\n",
    "    pred_xgb=xgr.predict(val_x)\n",
    "    \n",
    "    score_train = mean_absolute_error(train_y,pred_train_xgb)\n",
    "    scores_train.append(score_train)\n",
    "    score = mean_absolute_error(val_y,pred_xgb)\n",
    "    scores.append(score)\n",
    "\n",
    "print('Train mae:',np.mean(score_train))\n",
    "print('Val mae',np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3ï¼‰åˆ’åˆ†æ•°æ®é›†ï¼Œå¹¶ç”¨å¤šç§æ–¹æ³•è®­ç»ƒå’Œé¢„æµ‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict LR...\n",
      "Predict Ridge...\n",
      "Predict Lasso...\n",
      "Predict GBDT...\n",
      "{'learning_rate': 0.1, 'n_estimators': 80}\n"
     ]
    }
   ],
   "source": [
    "## Split data with val\n",
    "x_train,x_val,y_train,y_val = train_test_split(X_data,Y_data,test_size=0.3)\n",
    "\n",
    "## Train and Predict\n",
    "print('Predict LR...')\n",
    "model_lr = build_model_lr(x_train,y_train)\n",
    "val_lr = model_lr.predict(x_val)\n",
    "subA_lr = model_lr.predict(X_test)\n",
    "\n",
    "print('Predict Ridge...')\n",
    "model_ridge = build_model_ridge(x_train,y_train)\n",
    "val_ridge = model_ridge.predict(x_val)\n",
    "subA_ridge = model_ridge.predict(X_test)\n",
    "\n",
    "print('Predict Lasso...')\n",
    "model_lasso = build_model_lasso(x_train,y_train)\n",
    "val_lasso = model_lasso.predict(x_val)\n",
    "subA_lasso = model_lasso.predict(X_test)\n",
    "\n",
    "print('Predict GBDT...')\n",
    "model_gbdt = build_model_gbdt(x_train,y_train)\n",
    "val_gbdt = model_gbdt.predict(x_val)\n",
    "subA_gbdt = model_gbdt.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸€èˆ¬æ¯”èµ›ä¸­æ•ˆæœæœ€ä¸ºæ˜¾è‘—çš„ä¸¤ç§æ–¹æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict XGB...\n",
      "predict lgb...\n"
     ]
    }
   ],
   "source": [
    "print('predict XGB...')\n",
    "model_xgb = build_model_xgb(x_train,y_train)\n",
    "val_xgb = model_xgb.predict(x_val)\n",
    "subA_xgb = model_xgb.predict(X_test)\n",
    "\n",
    "print('predict lgb...')\n",
    "model_lgb = build_model_lgb(x_train,y_train)\n",
    "val_lgb = model_lgb.predict(x_val)\n",
    "subA_lgb = model_lgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sta inf of lgb:\n",
      "_min -126.864734992\n",
      "_max: 90152.4775557\n",
      "_mean 5917.96632163\n",
      "_ptp 90279.3422907\n",
      "_std 7358.88582391\n",
      "_var 54153200.5693\n"
     ]
    }
   ],
   "source": [
    "print('Sta inf of lgb:')\n",
    "Sta_inf(subA_lgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1ï¼‰åŠ æƒèåˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE of Weighted of val: 730.877443666\n",
      "Sta inf:\n",
      "_min -2816.93914153\n",
      "_max: 88576.7842223\n",
      "_mean 5920.38233546\n",
      "_ptp 91393.7233639\n",
      "_std 7325.20946801\n",
      "_var 53658693.7502\n"
     ]
    }
   ],
   "source": [
    "def Weighted_method(test_pre1,test_pre2,test_pre3,w=[1/3,1/3,1/3]):\n",
    "    Weighted_result = w[0]*pd.Series(test_pre1)+w[1]*pd.Series(test_pre2)+w[2]*pd.Series(test_pre3)\n",
    "    return Weighted_result\n",
    "\n",
    "## Init the Weight\n",
    "w = [0.3,0.4,0.3]\n",
    "\n",
    "## æµ‹è¯•éªŒè¯é›†å‡†ç¡®åº¦\n",
    "val_pre = Weighted_method(val_lgb,val_xgb,val_gbdt,w)\n",
    "MAE_Weighted = mean_absolute_error(y_val,val_pre)\n",
    "print('MAE of Weighted of val:',MAE_Weighted)\n",
    "\n",
    "## é¢„æµ‹æ•°æ®éƒ¨åˆ†\n",
    "subA = Weighted_method(subA_lgb,subA_xgb,subA_gbdt,w)\n",
    "print('Sta inf:')\n",
    "Sta_inf(subA)\n",
    "## ç”Ÿæˆæäº¤æ–‡ä»¶\n",
    "sub = pd.DataFrame()\n",
    "sub['SaleID'] = X_test.index\n",
    "sub['price'] = subA\n",
    "sub.to_csv('./sub_Weighted.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE of lr: 2597.45638384\n"
     ]
    }
   ],
   "source": [
    "## ä¸ç®€å•çš„LRï¼ˆçº¿æ€§å›å½’ï¼‰è¿›è¡Œå¯¹æ¯”\n",
    "val_lr_pred = model_lr.predict(x_val)\n",
    "MAE_lr = mean_absolute_error(y_val,val_lr_pred)\n",
    "print('MAE of lr:',MAE_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2ï¼‰Starkingèåˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Starking\n",
    "\n",
    "## ç¬¬ä¸€å±‚\n",
    "train_lgb_pred = model_lgb.predict(x_train)\n",
    "train_xgb_pred = model_xgb.predict(x_train)\n",
    "train_gbdt_pred = model_gbdt.predict(x_train)\n",
    "\n",
    "Strak_X_train = pd.DataFrame()\n",
    "Strak_X_train['Method_1'] = train_lgb_pred\n",
    "Strak_X_train['Method_2'] = train_xgb_pred\n",
    "Strak_X_train['Method_3'] = train_gbdt_pred\n",
    "\n",
    "Strak_X_val = pd.DataFrame()\n",
    "Strak_X_val['Method_1'] = val_lgb\n",
    "Strak_X_val['Method_2'] = val_xgb\n",
    "Strak_X_val['Method_3'] = val_gbdt\n",
    "\n",
    "Strak_X_test = pd.DataFrame()\n",
    "Strak_X_test['Method_1'] = subA_lgb\n",
    "Strak_X_test['Method_2'] = subA_xgb\n",
    "Strak_X_test['Method_3'] = subA_gbdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method_1</th>\n",
       "      <th>Method_2</th>\n",
       "      <th>Method_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39682.037093</td>\n",
       "      <td>41029.078125</td>\n",
       "      <td>40552.596813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>239.498371</td>\n",
       "      <td>266.032654</td>\n",
       "      <td>393.909761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6915.162439</td>\n",
       "      <td>7345.680664</td>\n",
       "      <td>7623.552178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11861.783785</td>\n",
       "      <td>11721.493164</td>\n",
       "      <td>11463.293245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>583.773267</td>\n",
       "      <td>513.307983</td>\n",
       "      <td>520.665295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Method_1      Method_2      Method_3\n",
       "0  39682.037093  41029.078125  40552.596813\n",
       "1    239.498371    266.032654    393.909761\n",
       "2   6915.162439   7345.680664   7623.552178\n",
       "3  11861.783785  11721.493164  11463.293245\n",
       "4    583.773267    513.307983    520.665295"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Strak_X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE of Stacking-LR: 628.399441036\n",
      "MAE of Stacking-LR: 707.673951794\n",
      "Predict Stacking-LR...\n"
     ]
    }
   ],
   "source": [
    "## level2-method \n",
    "model_lr_Stacking = build_model_lr(Strak_X_train,y_train)\n",
    "## è®­ç»ƒé›†\n",
    "train_pre_Stacking = model_lr_Stacking.predict(Strak_X_train)\n",
    "print('MAE of Stacking-LR:',mean_absolute_error(y_train,train_pre_Stacking))\n",
    "\n",
    "## éªŒè¯é›†\n",
    "val_pre_Stacking = model_lr_Stacking.predict(Strak_X_val)\n",
    "print('MAE of Stacking-LR:',mean_absolute_error(y_val,val_pre_Stacking))\n",
    "\n",
    "## é¢„æµ‹é›†\n",
    "print('Predict Stacking-LR...')\n",
    "subA_Stacking = model_lr_Stacking.predict(Strak_X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "subA_Stacking[subA_Stacking<10]=10  ## å»é™¤è¿‡å°çš„é¢„æµ‹å€¼\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['SaleID'] = TestA_data.SaleID\n",
    "sub['price'] = subA_Stacking\n",
    "sub.to_csv('./sub_Stacking.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sta inf:\n",
      "_min 10.0\n",
      "_max: 90849.3729816\n",
      "_mean 5917.39429976\n",
      "_ptp 90839.3729816\n",
      "_std 7396.09766172\n",
      "_var 54702260.6217\n"
     ]
    }
   ],
   "source": [
    "print('Sta inf:')\n",
    "Sta_inf(subA_Stacking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 ç»éªŒæ€»ç»“"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¯”èµ›çš„èåˆè¿™ä¸ªé—®é¢˜ï¼Œä¸ªäººçš„çœ‹æ³•æ¥è¯´å…¶å®æ¶‰åŠå¤šä¸ªå±‚é¢ï¼Œä¹Ÿæ˜¯æåˆ†å’Œæå‡æ¨¡å‹é²æ£’æ€§çš„ä¸€ç§é‡è¦æ–¹æ³•ï¼š\n",
    "\n",
    "* 1ï¼‰**ç»“æœå±‚é¢çš„èåˆ**ï¼Œè¿™ç§æ˜¯æœ€å¸¸è§çš„èåˆæ–¹æ³•ï¼Œå…¶å¯è¡Œçš„èåˆæ–¹æ³•ä¹Ÿæœ‰å¾ˆå¤šï¼Œæ¯”å¦‚æ ¹æ®ç»“æœçš„å¾—åˆ†è¿›è¡ŒåŠ æƒèåˆï¼Œè¿˜å¯ä»¥åšLogï¼Œexpå¤„ç†ç­‰ã€‚åœ¨åšç»“æœèåˆçš„æ—¶å€™ï¼Œæœ‰ä¸€ä¸ªå¾ˆé‡è¦çš„æ¡ä»¶æ˜¯æ¨¡å‹ç»“æœçš„å¾—åˆ†è¦æ¯”è¾ƒè¿‘ä¼¼ï¼Œç„¶åç»“æœçš„å·®å¼‚è¦æ¯”è¾ƒå¤§ï¼Œè¿™æ ·çš„ç»“æœèåˆå¾€å¾€æœ‰æ¯”è¾ƒå¥½çš„æ•ˆæœæå‡ã€‚\n",
    "\n",
    "* 2ï¼‰**ç‰¹å¾å±‚é¢çš„èåˆ**ï¼Œè¿™ä¸ªå±‚é¢å…¶å®æ„Ÿè§‰ä¸å«èåˆï¼Œå‡†ç¡®è¯´å¯ä»¥å«åˆ†å‰²ï¼Œå¾ˆå¤šæ—¶å€™å¦‚æœæˆ‘ä»¬ç”¨åŒç§æ¨¡å‹è®­ç»ƒï¼Œå¯ä»¥æŠŠç‰¹å¾è¿›è¡Œåˆ‡åˆ†ç»™ä¸åŒçš„æ¨¡å‹ï¼Œç„¶ååœ¨åé¢è¿›è¡Œæ¨¡å‹æˆ–è€…ç»“æœèåˆæœ‰æ—¶ä¹Ÿèƒ½äº§ç”Ÿæ¯”è¾ƒå¥½çš„æ•ˆæœã€‚\n",
    "\n",
    "* 3ï¼‰**æ¨¡å‹å±‚é¢çš„èåˆ**ï¼Œæ¨¡å‹å±‚é¢çš„èåˆå¯èƒ½å°±æ¶‰åŠæ¨¡å‹çš„å †å å’Œè®¾è®¡ï¼Œæ¯”å¦‚åŠ Stakingå±‚ï¼Œéƒ¨åˆ†æ¨¡å‹çš„ç»“æœä½œä¸ºç‰¹å¾è¾“å…¥ç­‰ï¼Œè¿™äº›å°±éœ€è¦å¤šå®éªŒå’Œæ€è€ƒäº†ï¼ŒåŸºäºæ¨¡å‹å±‚é¢çš„èåˆæœ€å¥½ä¸åŒæ¨¡å‹ç±»å‹è¦æœ‰ä¸€å®šçš„å·®å¼‚ï¼Œç”¨åŒç§æ¨¡å‹ä¸åŒçš„å‚æ•°çš„æ”¶ç›Šä¸€èˆ¬æ˜¯æ¯”è¾ƒå°çš„ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5-æ¨¡å‹èåˆ END.**\n",
    "\n",
    "--- By: ML67 \n",
    "\n",
    "        Email: maolinw67@163.com\n",
    "        PS: åä¸­ç§‘æŠ€å¤§å­¦ç ”ç©¶ç”Ÿ, é•¿æœŸæ··è¿¹Tianchiç­‰ï¼Œå¸Œæœ›å’Œå¤§å®¶å¤šå¤šäº¤æµã€‚\n",
    "        github: https://github.com/mlw67 ï¼ˆè¿‘æœŸä¼šåšä¸€äº›ä¹¦ç±æ¨å¯¼å’Œä»£ç çš„æ•´ç†ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**å…³äºDatawhaleï¼š**\n",
    "\n",
    "> Datawhaleæ˜¯ä¸€ä¸ªä¸“æ³¨äºæ•°æ®ç§‘å­¦ä¸AIé¢†åŸŸçš„å¼€æºç»„ç»‡ï¼Œæ±‡é›†äº†ä¼—å¤šé¢†åŸŸé™¢æ ¡å’ŒçŸ¥åä¼ä¸šçš„ä¼˜ç§€å­¦ä¹ è€…ï¼Œèšåˆäº†ä¸€ç¾¤æœ‰å¼€æºç²¾ç¥å’Œæ¢ç´¢ç²¾ç¥çš„å›¢é˜Ÿæˆå‘˜ã€‚Datawhale ä»¥â€œfor the learnerï¼Œå’Œå­¦ä¹ è€…ä¸€èµ·æˆé•¿â€ä¸ºæ„¿æ™¯ï¼Œé¼“åŠ±çœŸå®åœ°å±•ç°è‡ªæˆ‘ã€å¼€æ”¾åŒ…å®¹ã€äº’ä¿¡äº’åŠ©ã€æ•¢äºè¯•é”™å’Œå‹‡äºæ‹…å½“ã€‚åŒæ—¶ Datawhale ç”¨å¼€æºçš„ç†å¿µå»æ¢ç´¢å¼€æºå†…å®¹ã€å¼€æºå­¦ä¹ å’Œå¼€æºæ–¹æ¡ˆï¼Œèµ‹èƒ½äººæ‰åŸ¹å…»ï¼ŒåŠ©åŠ›äººæ‰æˆé•¿ï¼Œå»ºç«‹èµ·äººä¸äººï¼Œäººä¸çŸ¥è¯†ï¼Œäººä¸ä¼ä¸šå’Œäººä¸æœªæ¥çš„è”ç»“ã€‚\n",
    "\n",
    "æœ¬æ¬¡æ•°æ®æŒ–æ˜è·¯å¾„å­¦ä¹ ï¼Œä¸“é¢˜çŸ¥è¯†å°†åœ¨å¤©æ± åˆ†äº«ï¼Œè¯¦æƒ…å¯å…³æ³¨Datawhaleï¼š\n",
    "\n",
    "![](http://jupter-oss.oss-cn-hangzhou.aliyuncs.com/public/files/image/2326541042/1584426326920_9FOUExG2be.jpg)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
